#+TITLE: What Are Types?


* Computational Type Theory

Robert Constable, in his article "Computational Type Theory"
(cite:constable09_comput_type_theor):

#+BEGIN_QUOTE
Computational type theory [CTT] answers questions such as: What is a type? What is a
natural number? How do we compute with types? How are types related to sets? How
are data types for numbers, lists, trees, graphs, etc. related to the
corresponding notions in mathematics? Do paradoxes arise in formulating a theory
of types as they do in formulating a theory of sets?
#+END_QUOTE

The first question is decisive for CTT's answers to the all the rest, and it is
the focus of this section. We would like to articulate what a type is from the
perspective of "type theory" in general, but we begin by looking at CTT in
particular.

In formulating his account of types (for a popular audience), Constable drives
the question "what is a type?" back to the question "what is a term?":

#+BEGIN_QUOTE
What is a type? To answer this question, we must first be more precise about
computation; we will see that the notion of a type is ultimately grounded in
computation, specifically in concrete linguistic expressions because computation
in the physical world is ultimately symbolic. We are only interested in
computation that is physically realizable by explicit, verifiable human actions
and by machines that humans understand. Thus to explain types, we first need to
explain terms and how to compute with them.

(cite:constable09_comput_type_theor)
#+END_QUOTE

# TODO Is this a correct reading?

# Note that this route goes, first, through a semiotic reduction ("computation in
# the physical world is ultimately symbolic"), then through a physicalistic
# reduction ("we are only interested in computation that is physically
# realizable"), and finally through a phenomenological reduction ("realizable by
# explicit, verifiable human actions and by machines that humans understand") --
# that this is phenomenological is not immediately evident here, but it will be
# made clearer in what follows.

Let us fix some set of terms (symbols), $T$, and an /operational semantics/,
telling us how to compute certain elements $t_1, ..., t_n \in T$ from other
elements $t'_1, ..., t'_n \in T$, via the relation $\to$, such that $t' \to t$
reads "$t'$ reduces to $t$". We say a term in $T$ is "irreducible" if it is only
reducible to itself: $t \to t$. Such a term is called "canonical". That this
sets up a (partial) ordering over a set of /signs/.[fn:rewriting]

We might expect that more complex signs reduce to simpler ones, and this is
often the case, but we haven't actually placed any constraint that enforces this
kind converge upon atomic elements. The canonical signs serve as points of rest,
in our system, but there is no requirement enrcing in the sense that they cannot
be further reduced to simpler components.

Having established such a system of signs, we define /types/ over the canonical
elements by delimiting collections of terms that belong together, and
stipulating how to determine if any two terms in such a collection are to
taken as equal:

#+BEGIN_QUOTE
To define a type we specify a collection of canonical terms which are the
canonical elements of the type, and we define an equality relation declaring
when two canonical terms denote the same abstract object. The equality relation
creates abstract objects out of terms.

(cite:constable09_comput_type_theor)
#+END_QUOTE

Note that these types presuppose the /construction of ideal/ (abstract)
/objects/, on the basis of positing /equality between concrete objects/: "*The
equality relation creates abstract objects out of terms.*". Existence precedes
essence, and essence is "created" through acts of /gathering/ (terms that belong
together) and /equalization/ (of terms that are taken to be the same).
# TODO C.f., Frega /Über Sinn und Bedeutung/.

* A Short Genealogy of Types

The roots of type theories, computational or otherwise, extend back through the
history of philosophy, at least to Aristotle (cite:constable09_comput_type_theor).

** Prehistory of the Theories of Types

#+BEGIN_QUOTE
In all subsumptions of an object under a concept the representations of the
former must be *homogeneous* [Gleichartig] with the latter, i.e., the concept
must contain that which is represented in the object that is to be subsumed
under it, for that is just what is meant by the expression "an object is
contained under a concept".

(cite:kant98_critiq, A137/B176)
#+END_QUOTE

** Dawn of the Theories of Types

Martin-Löf's, in his "An intuitionistic theory of types", explicitly traces his
concept of type back to Russel:

#+BEGIN_QUOTE
Every mathematical object is of a certain kind or /type/. Better, a mathematical
object is always given together with its type, that is, it is not just an
object, it is an object of a certain type. This may be regarded as a simpler and
at the same time more general formulation of Russel's 1903 /doctrine of types/,
according to which a type is the range of significance of a propositional
function, because in the theory that I am about to describe every propositional
function will indeed have a type as its domain.

(cite:martin1998intuitionistic)
#+END_QUOTE

Russell's theory of types puts the concept of a propositional /function/ *under*
the concept of a general proposition ("under" here meaning "conceptually prior
to").

#+BEGIN_QUOTE
Every proposition containing 'all' asserts that some propositional function is
always true; and this means that all values of the said function are true
#+END_QUOTE

Then the meaning of quantification is subordinate to the range of /valid/ values
of a function.

#+BEGIN_QUOTE
Hence we can speak of all of a collection when and only when the collection
forms part or the whole of the range of significance of some propositional
function, the range of significance being defined as the collection of those
arguments for which the function in question is significant, i. e., has a value.
(cite:10.2307/2369948)
#+END_QUOTE

Hence, "a type is defined as the range of significance of a propositional
function, i.e., as the collection of arguments for which the said function has
values" (cite:10.2307/2369948).

But what determines the /validity/ of this range? Russell names it "logical
homogeneity":

#+BEGIN_QUOTE
What is essential, as appears from the above discussion, is not finitude, but
what may be called logical homogeneity. This property is to belong to any
collection whose terms are all contained within the range of significance of
some one function. It would always be obvious at a glanice whether a collection
possessed this property or not, if it were not for the concealed ambiguity in
common logical terms such as true and false, which gives an appearance of being
a single function to what is really a conglomeration of many functions with
different ranges of significance.

(cite:10.2307/2369948)
#+END_QUOTE

/Logical homogeneity/ is a property of /collections of terms/. A collection of
terms has this property when all the terms are "contained within the range of
significance of some one function". A "type" is then a special case of a
logically homogenius collection of terms: a logically homogenius collection of
terms whose characteristic function is /propositional/ is a /type/.

Of course, this all leaves undecided the really essential and most interesting
question: How is the homogeneity of such collections determined and enforced? By
virtue of what can we say that the terms in such a collection /belong/ together?
And, once we've determined that belonging together, how do we enforce this?

* Metaphysical Foundations of the Computational Theory of Types
** TODO Equality
** TODO Belonging together

The types of the typed λ-calculi are /marks/, or /indexes/, used to enforce a
sense of appropriateness that follows from presumptions of /belonging together/.

This is indeed how types function in the calculi when represented ala Church.
The types mark the λ-terms themselves, being inscribed into each head. These
marks serve as a prophylactic protecting against unwanted applications. This
usage echos the etymon of 'type':

#+BEGIN_QUOTE
late 15c., "symbol, emblem," from Latin /typus/ "figure, image, form, kind,"
from Greek /typos/ "a blow, dent, impression, mark, effect of a blow; figure in
relief, image, statue; anything wrought of metal or stone; general form,
character; outline, sketch," from root of /typtein/ "to strike, beat," from PIE
/*tup-/, variant of root /*(s)teu-/ (1) "to push, stick, knock, beat"
([[https://www.etymonline.com/search?q=type][etymonline]])
#+END_QUOTE

*** Justification:

What justifies such enforcement?
* Types as Theories

In /Types as Theories/, Goguen denies that "type theory" advances a general
theory of types, in arguing instead that it presents a theory based on a
specific, limited notion of "type":

#+BEGIN_QUOTE
In the "types as predicates" variant of the "types as sets" approach, types are
taken to be predicates, which therefore denote sets (or some variant thereof,
such as domains). However, many advocates of this view are more proof
theoretically inclined, and hence might resist such denotations. Perhaps the
best known work along this line is Martin-Löf's "type theory", which also
provides dependent types, as implemented in Pebble and other languages. (Note
that "type theory" is not a general theory of types, but rather a specific
intuitionistic logic which provides one specific notion of type).

(cite:Programming91typesas)
#+END_QUOTE

Goguen counters the "types as predicates" view with another interpretation,
which he calls "types as theories" or "types as algebras":

#+BEGIN_QUOTE
The essential insight of the "types as algebras" notion is that the /operations/
associated with data are at least as important as the values. Thus, the this
approach generalized from [types as] sets to /algebras/, which are just sets
with some given operations.

(cite:Programming91typesas)
#+END_QUOTE

Let's lean on the proof-theory side of types-as-propositions and recall (what I
believe to be) a key insight of Gentzen's approach:

#+BEGIN_QUOTE
To every logical symbol ... belongs precisely one inference figure which
'introduces' the symbol - as the terminal symbol of a formula - and one which
'eliminates' it. ... The introductions represent, as it were, the 'definitions'
of the symbols concerned, and the eliminations are no more than, in the final
analysis, than the consequences of these definitions. This fact may be expressed
as follows: In eliminating a symbol, we may use the formula with whose terminal
symbol we are dealing only 'in the sense afforded by the introduction of that
symbol'.

cite:m.69_gerhar_gentz_german
#+END_QUOTE

That is, the meaning of the logical connectives (i.e., the correspondents of the
principle types in the various type theories) is given by their introduction and
elimination rules. From the Curry perspective on typing, I think we can say that
the intro/elim rules are (partial) operations on the sets of derivations in the
lambda calculus (since a set of derivations is a set of programs, which is a set
of proofs, which is the meaning of a proposition according to the perspective of
proof-theoretic semantics). If this is correct, I think we have a perspective
from which we can say that the meaning of the types in our systems are indeed
given by the operations belonging to each type (i.e., the particular sets of
operations which carve out patterns of permitted connections within the sets of
derivations of the lambda calculus).

* Footnotes

[fn:rewriting] We sketch out the barest form of a term-rewriting system here,
to sidestep the complications introduced by the usual pratice of using the
lambda calculus in particular. There's good reason to favor [[https://en.wikipedia.org/wiki/Cartesian_closed_category][typed lambda calculi]]
(thanks to [[https://boarders.github.io/][Callan McGill]] for explaining this reason to me, when I asked, very
naively, "Why use lambda calculus instead of some other rewrite system?"), but
they're not important for the quite rudimentary level of understanding I'm
aiming for here.
